# ğŸ“Š Real-Time Crypto & Financial Market Stream with Kafka, Spark & PostgreSQL

ğŸš€ Projet Big Data complet simulant un pipeline temps rÃ©el de donnÃ©es financiÃ¨res (cryptomonnaies & marchÃ©s boursiers) avec ingestion Kafka, traitement Spark, stockage PostgreSQL et visualisation Power BI.

---

## ğŸ¯ Objectif pÃ©dagogique

Comprendre et mettre en Å“uvre un **pipeline de streaming temps rÃ©el** basÃ© sur :
- Ingestion de flux API (crypto & actions)
- Traitement distribuÃ© avec Spark Structured Streaming
- Stockage structurÃ© dans PostgreSQL
- Visualisation mÃ©tier dans Power BI

ğŸ“ Ce projet sâ€™inspire des cas concrets rencontrÃ©s en entreprise pour te prÃ©parer au monde professionnel (Data Engineering / Streaming).

---

## âš™ï¸ Stack Technique

| Composant             | RÃ´le                                       |
|----------------------|--------------------------------------------|
| **Kafka / Zookeeper**| Ingestion de flux temps rÃ©el               |
| **Spark Streaming**  | Traitement des flux Kafka                  |
| **PostgreSQL**       | Stockage des donnÃ©es transformÃ©es          |
| **Docker**           | Containerisation du cluster Big Data       |
| **Power BI**         | Dashboard de visualisation en temps rÃ©el   |
| **APIs utilisÃ©es**   | CryptoCompare (crypto), Finnhub (actions)  |

---

## ğŸ§± Architecture du Projet

```
producers/       # Scripts producteurs Kafka (API â†’ Kafka)
consumers/       # Scripts consommateurs Kafka (Kafka â†’ PostgreSQL)
utils/           # Fichiers utilitaires (connexion DB, Kafka)
notebooks/       # Analyse exploratoire & traitement Spark
docs/            # Ã‰tapes, configurations et documentation
.env             # Variables d'environnement (non versionnÃ©)
start_pipeline.bat  # Script de lancement automatique (Windows)
```

---

## ğŸš€ Pipeline Ã‰tape par Ã‰tape

1. **ğŸ§± Mise en place du cluster DockerisÃ©**
   - RÃ©seau dÃ©diÃ© `spark-cluster`
   - Services : Kafka, Zookeeper, Spark master/worker, PostgreSQL

2. **ğŸ”Œ Ingestion de donnÃ©es via API**
   - `producer_crypto.py` interroge CryptoCompare (BTC, ETHâ€¦)
   - `producer_stocks.py` interroge Finnhub (AAPL, MSFTâ€¦)
   - Les donnÃ©es sont envoyÃ©es dans `crypto_topic` et `stock_topic`

3. **ğŸ”¥ Traitement Spark Streaming**
   - Lecture temps rÃ©el des topics Kafka
   - Parsing JSON, transformation & agrÃ©gation

4. **ğŸ—„ Stockage dans PostgreSQL**
   - Connexion via JDBC
   - Tables `crypto_data` et `stock_data` alimentÃ©es en direct

5. **ğŸ“ˆ Visualisation avec Power BI**
   - Connexion DirectQuery Ã  PostgreSQL
   - Dashboard : prix, volumes, tendances

---

## ğŸ“¦ Bonnes pratiques

- Variables sensibles gÃ©rÃ©es via `.env` et `python-dotenv`
- Scripts rÃ©utilisables et modulaires (`db_utils.py`, `kafka_utils.py`)
- Lancement automatisÃ© avec `start_pipeline.bat`
- Projet structurÃ© par rÃ´le : ingestion, traitement, stockage, visualisation

---

## âœ… CompÃ©tences mises en pratique

- ğŸ“Œ Spark Structured Streaming
- ğŸ“Œ Kafka Topics & Producers/Consumers Python
- ğŸ“Œ Connexion Python â†” PostgreSQL
- ğŸ“Œ Containerisation Big Data avec Docker
- ğŸ“Œ RequÃªtage SQL pour Power BI

---

## ğŸ‘¨â€ğŸ’» Auteur

**Moctarr Basiru King Rahman**  
ğŸ“ Ã‰tudiant MastÃ¨re Data Engineering â€“ ECE Paris  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/moctarr)  
ğŸ“§ [Email](mailto:moctarrbasiru.kingrahman@edu.ece.fr)

---

## ğŸ“„ Licence

MIT License â€“ libre rÃ©utilisation pour but pÃ©dagogique et dÃ©monstratif.